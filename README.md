# Spacecraft Pose Estimation Framework (SPEF)

The **Spacecraft Pose Estimation Framework (SPEF)** is an open-source toolkit for **training, quantizing, and deploying deep neural networks** for spacecraft pose estimation from monocular images and video sequences.

SPEF has been developed in the context of a Ph.D. thesis and an upcoming IEEE TAES journal publication. It supports **real-time embedded implementations** on **CPU**, **GPU (NVIDIA Jetson)**, and **FPGA**, and includes utilities for **dataset generation**, **temporal evaluation**, and **visualization**.

---

## ðŸš€ Features

- **Pose Estimation Training**
- **Quantization (QAT)**:
  - **Brevitas + FINN** for FPGA
  - **PyTorch FX** for CPU
  - **pytorch-quantization-toolkit** for NVIDIA Jetson GPU
- **Embedded Deployment**:
  - CPU with TVM
  - GPU (Jetson) with TensorRT
  - FPGA (Xilinx UltraScale+) with FINN
- **Graphical User Interface (GUI)** for real-time pose visualization
- **Dataset Generation Tools** for D-SPEED dataset creation

---

## ðŸ“¦ Installation (via Docker)

All installations are handled via Dockerfiles located in the `setup/` directory. Please refer to the individual README files in `setup/` for detailed build instructions and preparation steps for each target platform (**CPU**, **Jetson GPU**, and **FPGA**).

---

## ðŸ’¾ Pretrained Models and Experiments

The pretrained models and experimental results are hosted on Zenodo due to file size limitations.  

ðŸ‘‰ [Download from Zenodo](https://doi.org/10.5281/zenodo.17137746)  

Contents on Zenodo:  
- **models/**: pretrained neural network weights and checkpoints, evaluation metrics, configuration files, and annotated 3D keypoints of the *Tango* spacecraft.  
- **experiments/**: training logs, model compilation outputs, embedded execution metrics, temporal filtering experiments, and ablation studies on probabilistic encoding (soft classification).  
- **finn_build/** *(optional)*: intermediate build folders generated by **FINN**. These are not required for running experiments, but can be useful if you want to directly inspect the build artifacts without regenerating them.

---

## ðŸ›  Usage

### 1. Training (Brevitas docker image)
Before launching training, go to `src/config/train/` and create your YAML configuration files:
- For **Brevitas models (QAT)** with mixed precision: create a folder `exp_X` (e.g., `exp_0`, `exp_1`) containing the YAML configuration file and JSON bit-width file.
- For **PyTorch models (Float32)**: create a single file `exp_X.yaml` (e.g., `exp_1.yaml`).

Training results will be stored in `experiments/train/`.
```bash
python train.py
```

### 2. Model Building (Use the corresponding Docker image for each target platform)
**GPU/CPU build pipeline**: ðŸŽ¯ QAT â†’ ðŸ› ï¸ Compile â†’ ðŸ“Š Evaluate â†’ ðŸ’¾ Store results:
- **GPU (NVIDIA)**: `build_nvidia.py` â†’ configs in `src/build/nvidia/` â†’ results in `experiments/build/nvidia/`
- **CPU (TVM)**: `build_tvm.py` â†’ configs in `src/build/tvm/` â†’ results in `experiments/build/tvm/`

**FPGA build pipeline** (QAT must be run manually with the Brevitas Docker image â€” see Step 1): ðŸ› ï¸ Compile â†’ ðŸ’¾ Store results :
- **FPGA (FINN)**: `build_finn.py` â†’ configs in `src/build/finn/` â†’ results in `experiments/build/finn/`

> For more details about the FPGA implementation, please refer to our dedicated repository: **[FPGA-SpacePose](https://github.com/possoj/FPGA-SpacePose)**

### 3. Deployment (Use the corresponding Docker image for each target platform)
Deployment is automatic.  
Make sure to follow the steps in the `setup/` directory to configure the board (install OS, libraries, etc.).  
Adapt the **IP address**, **port**, **username**, and **password** in `src/boards/boards_cfg.py` according to your setup.

Then run the corresponding deployment script:
- **CPU (TVM)**: `deploy_cpu.py`  
- **GPU (NVIDIA)**: `deploy_nvidia.py`  
- **FPGA (FINN)**: `deploy_fpga.py`

**Note â€” selecting the experiment to deploy**  
When running `deploy_cpu.py` (same for `deploy_nvidia.py` / `deploy_fpga.py`), the script will prompt you for the **experiment** to deploy. This corresponds to the folder created during the *build* phase, e.g.:  
- TVM: `experiments/build/tvm/<run_id>`  
- NVIDIA: `experiments/build/nvidia/<run_id>`  
- FINN: `experiments/build/finn/<run_id>`  

### 4. Additional Tools
- `temporal.py`: Evaluate on temporal D-SPEED data â†’ results in `experiments/temporal/`
- `nn_stats.py`: Print per-layer network statistics (MACs, parameters)
- `soft_class_plot.py`: Evaluate soft-classification encoding/decoding impact on pose accuracy â†’ results in `experiments/soft_classification/`

### 5. GUI
```bash
xhost +
python gui.py
```
If the Jetson board has been configured and is reachable from the host computer (see Step 3 â€” Deployment), you can also deploy to it directly through the GUI.

---

## ðŸ“‚ Repository Structure
```
Spacecraft-Pose-Estimation-Framework/
â”œâ”€â”€ experiments/        # Stores all experiment results (training, build, temporal, etc.)
â”œâ”€â”€ models/             # Manually copy models here after training for deployment or GUI visualization
â”œâ”€â”€ setup/              # Dockerfiles and platform-specific README instructions
â”œâ”€â”€ src/                # Source code
â”œâ”€â”€ finn_build/         # Intermediate results/debug folder used by FINN
â”œâ”€â”€ train.py            # Training entry point
â”œâ”€â”€ eval.py             # Evaluation entry point
â”œâ”€â”€ build_finn.py       # Build script for FPGA (FINN)
â”œâ”€â”€ build_nvidia.py     # Build script for NVIDIA Jetson GPU (TensorRT)
â”œâ”€â”€ build_tvm.py        # Build script for CPU (TVM)
â”œâ”€â”€ deploy_finn.py      # Deployment script for FPGA
â”œâ”€â”€ deploy_nvidia.py    # Deployment script for NVIDIA Jetson GPU
â”œâ”€â”€ deploy_tvm.py       # Deployment script for CPU
â”œâ”€â”€ gui.py              # Graphical User Interface for pose visualization
â”œâ”€â”€ temporal.py         # Evaluation on temporal data from D-SPEED, outputs to experiments/temporal
â”œâ”€â”€ nn_stats.py         # Per-layer network stats including MAC count and parameters
â”œâ”€â”€ soft_class_plot.py  # Experiment to visualize impact of soft-classification encoding/decoding on pose accuracy
â”œâ”€â”€ README.md           # This file
â””â”€â”€ LICENSE             # MIT License
```

---

## ðŸ“Š Datasets

This repository supports multiple spacecraft pose estimation datasets, including:

### [D-SPEED: Dynamicâ€“Spacecraft Pose Estimation Dataset](https://zenodo.org/records/15851302)

A large-scale dataset of **60k labeled still images** and **21 annotated video sequences (25 FPS)** for spacecraft pose estimation.  
It is designed to evaluate both **single-frame** and **temporal** approaches.

[![Watch the video](https://img.youtube.com/vi/AbIYOj8LuNY/maxresdefault.jpg)](https://youtu.be/AbIYOj8LuNY?si=wQPHpHEwogIYLbT4)

### SPEED Dataset Splits

Predefined **train/validation splits** of the original SPEED dataset are provided in: `src/data/datasets/speed_split`

---

## ðŸ“ˆ Results (Mobile-URSONet+ embedded)

| Platform             | Pose error | FPS  | Power   |
|----------------------|------------|------|---------|
| CPU (ARM)            | 0.2208     | 12.1 | 1.22 W  |
| Jetson Orin Nano     | 0.2088     | 560  | 4.28 W  |
| FPGA UltraScale+     | 0.3518     | 58.7 | 0.865 W |

Full details will be provided in the IEEE TAES article (coming soon, currently under review).

---

## ðŸ“œ License
[MIT License](LICENSE)

---

## ðŸ‘¤ Author
Julien Posso â€“ Ph.D. in Computer Engineering, Polytechnique MontrÃ©al

---

## ðŸ“¬ Contact

julien.posso@gmail.com / julien.posso@polymtl.ca
